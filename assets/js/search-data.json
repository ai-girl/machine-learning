{
  
    
        "post0": {
            "title": "Advanced Certification Program in Computational Data Science",
            "content": "Learning Objectives . At the end of the mini-project, you will be able to : . perform data preprocessing, EDA and feature extraction on the Resume dataset | perform multinomial Naive Bayes classification on the Resume dataset | . Dataset description . The data is in CSV format, with two features: Category, and Resume. . Category - Industry sector to which the resume belongs to, and . Resume - The complete CV (text) of the candidate. . Grading = 10 Points . Information . Companies often receive thousands of resumes for each job posting and employ dedicated screening officers to screen qualified candidates. Finding suitable candidates for an open role from a database of 1000s of resumes can be a tough task. Automated resume categorization can speeden the candidate selection process. Such automation can really ease the tedious process of fair screening and shortlisting the right candidates and aid quick decisionmaking. . To learn more about this, click here. . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import warnings warnings.filterwarnings(&#39;ignore&#39;) import regex as re from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder from sklearn.naive_bayes import MultinomialNB from sklearn import metrics from sklearn.metrics import accuracy_score from pandas.plotting import scatter_matrix from sklearn import metrics from sklearn.feature_extraction.text import TfidfVectorizer from matplotlib.gridspec import GridSpec from bs4 import BeautifulSoup import nltk nltk.download(&#39;stopwords&#39;) nltk.download(&#39;punkt&#39;) from nltk.corpus import stopwords import string from wordcloud import WordCloud . [nltk_data] Downloading package stopwords to /root/nltk_data... [nltk_data] Package stopwords is already up-to-date! [nltk_data] Downloading package punkt to /root/nltk_data... [nltk_data] Package punkt is already up-to-date! . Downloading the data . !wget -qq https://cdn.iisc.talentsprint.com/CDS/Datasets/UpdatedResumeDataSet.csv print(&quot;Data Downloaded Successfuly!!&quot;) . Data Downloaded Successfuly!! . Exercise 1: Read the UpdatedResumeDataset.csv dataset [0.5 Mark] . Hint: pd.read_csv() . # YOUR CODE HERE resume_df = pd.read_csv(&quot;https://cdn.iisc.talentsprint.com/CDS/Datasets/UpdatedResumeDataSet.csv&quot;) print(resume_df.head()) . Category Resume 0 Data Science Skills * Programming Languages: Python (pandas... 1 Data Science Education Details r nMay 2013 to May 2017 B.E... 2 Data Science Areas of Interest Deep Learning, Control Syste... 3 Data Science Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table... 4 Data Science Education Details r n MCA YMCAUST, Faridab... . resume_df.loc[:,&quot;Category&quot;].unique() . array([&#39;Data Science&#39;, &#39;HR&#39;, &#39;Advocate&#39;, &#39;Arts&#39;, &#39;Web Designing&#39;, &#39;Mechanical Engineer&#39;, &#39;Sales&#39;, &#39;Health and fitness&#39;, &#39;Civil Engineer&#39;, &#39;Java Developer&#39;, &#39;Business Analyst&#39;, &#39;SAP Developer&#39;, &#39;Automation Testing&#39;, &#39;Electrical Engineering&#39;, &#39;Operations Manager&#39;, &#39;Python Developer&#39;, &#39;DevOps Engineer&#39;, &#39;Network Security Engineer&#39;, &#39;PMO&#39;, &#39;Database&#39;, &#39;Hadoop&#39;, &#39;ETL Developer&#39;, &#39;DotNet Developer&#39;, &#39;Blockchain&#39;, &#39;Testing&#39;], dtype=object) . Pre-processing and EDA . Exercise 2: Display all the categories of resumes and their counts in the dataset [0.5 Mark] . # YOUR CODE HERE print(&quot;Distinct Categories of resume:&quot;) print(resume_df.loc[:,&quot;Category&quot;].unique()) . Distinct Categories of resume: [&#39;Data Science&#39; &#39;HR&#39; &#39;Advocate&#39; &#39;Arts&#39; &#39;Web Designing&#39; &#39;Mechanical Engineer&#39; &#39;Sales&#39; &#39;Health and fitness&#39; &#39;Civil Engineer&#39; &#39;Java Developer&#39; &#39;Business Analyst&#39; &#39;SAP Developer&#39; &#39;Automation Testing&#39; &#39;Electrical Engineering&#39; &#39;Operations Manager&#39; &#39;Python Developer&#39; &#39;DevOps Engineer&#39; &#39;Network Security Engineer&#39; &#39;PMO&#39; &#39;Database&#39; &#39;Hadoop&#39; &#39;ETL Developer&#39; &#39;DotNet Developer&#39; &#39;Blockchain&#39; &#39;Testing&#39;] . # YOUR CODE HERE print(&quot; All the categories of resumes and their counts:&quot;) resume_df.loc[:,&quot;Category&quot;].value_counts() . All the categories of resumes and their counts: . Java Developer 84 Testing 70 DevOps Engineer 55 Python Developer 48 Web Designing 45 HR 44 Hadoop 42 Blockchain 40 ETL Developer 40 Operations Manager 40 Data Science 40 Sales 40 Mechanical Engineer 40 Arts 36 Database 33 Electrical Engineering 30 Health and fitness 30 PMO 30 Business Analyst 28 DotNet Developer 28 Automation Testing 26 Network Security Engineer 25 SAP Developer 24 Civil Engineer 24 Advocate 20 Name: Category, dtype: int64 . Exercise 3: Create the count plot of different categories [0.5 Mark] . Hint: Use sns.countplot() . plt.figure(figsize=(20,7)) sns.countplot(x=&quot;Category&quot;, data=resume_df) plt.xticks(rotation=45) plt.show() . Exercise 4: Create a pie plot depicting the percentage of resume distributions category-wise [0.5 mark] . Hint: Use plt.pie() and plt.get_cmap for color mapping the pie chart. . targetCounts = resume_df[&#39;Category&#39;].value_counts() targetLabels = resume_df[&#39;Category&#39;].unique() # Make square figures and axes #plt.figure(1, figsize=(8,8)) #the_grid = GridSpec(2, 2) # YOUR CODE HERE to display pie chart with color coding (eg. `coolwarm`) #cmap=plt.get_cmap(name) fig1, ax1 = plt.subplots(figsize=(10, 10)) #fig1.subplots_adjust(0.3, 0, 1, 1) theme = plt.get_cmap(&#39;coolwarm&#39;) ax1.set_prop_cycle(&quot;color&quot;, [theme(1. * i / len(targetCounts)) for i in range(len(targetCounts))]) ax1.pie([x*100 for x in targetCounts],labels=[x for x in targetLabels],autopct=&#39;%0.1f&#39;) plt.title(&#39;Percentage resumes per category&#39;) plt.show() . Exercise 5: Convert all the Resume text to lower case [0.5 Mark] . # YOUR CODE HERE resume_df.iloc[:,1] = resume_df.iloc[:,1].str.lower() . Cleaning resumes&#39; text data . Exercise 6: Define a function to clean the resume text [2 Mark] . In the text there are special characters, urls, hashtags, mentions, etc. Remove the following: . URLs: For reference click here | RT | cc: For reference click here | Hashtags, # and Mentions, @ | punctuations | extra whitespace | . PS: Use the provided reference similarly for removing any other such elements. . After cleaning as above, store the Resume Text in a separate column (New Feature). . def remove_special_characters(text, remove_digits=True): pattern = r&#39;[^a-zA-z0-9 s]&#39; text = re.sub(pattern,&#39;&#39;,text) return text # removing the html strips def strip_html(text): # BeautifulSoup is a useful library for extracting data from HTML and XML documents soup = BeautifulSoup(text, &quot;html.parser&quot;) return soup.get_text() # removing the square brackets def remove_between_square_brackets(text): return re.sub(&#39; [[^]]* ]&#39;, &#39;&#39;, text) # removing the noisy text def denoise_text(text): text = strip_html(text) # YOUR CODE HERE to create &#39;text&#39; by applying remove_between_square_brackets() to &#39;text&#39; text = remove_between_square_brackets(text) # YOUR CODE HERE to return text return text def remove_hastags_mentions_ulrs(text): # removing mentions text = re.sub(&quot;@ S+&quot;, &quot;&quot;, text) # remove market tickers text = re.sub(&quot; $&quot;, &quot;&quot;, text) # remove urls text = re.sub(&quot;https?: / /.*[ r n]*&quot;, &quot;&quot;, text) # removing hashtags text = re.sub(&quot;#&quot;, &quot;&quot;, text) return text def simple_stemmer(text): ps = nltk.porter.PorterStemmer() text = &#39; &#39;.join([ps.stem(word) for word in text.split()]) return text def remove_punctuation_and_line_breaks(text): # Remove puncuation #translator = str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation) text = text.translate(str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation)) # Removiing Non-ASCII Unicode # encoding the text to ASCII format text= text.encode(encoding=&quot;ascii&quot;, errors=&quot;ignore&quot;) # decoding the text text = text.decode() # cleaning the text to remove extra whitespace text = &quot; &quot;.join([word for word in text.split()]) # Remove line breaks #text = re.sub(r&#39; n&#39;, &#39;&#39;, text) return text def data_cleaning(text): text = remove_special_characters(text) text = denoise_text(text) text = remove_hastags_mentions_ulrs(text) text = remove_punctuation_and_line_breaks(text) text = simple_stemmer(text) return text . # YOUR CODE HERE resume_df[&#39;CleanedResume&#39;] = resume_df[&#39;Resume&#39;].apply(data_cleaning) . sent_lens = [] for i in resume_df.CleanedResume: length = len(i.split()) sent_lens.append(length) print(len(sent_lens)) print(max(sent_lens)) . 962 2007 . Stopwords removal . The stopwords, for example, and, the, was, and so forth etc. appear very frequently in the text and are not helpful in the predictive process. Therefore these are usually removed for text analytics and text classification purposes. . Tokenize the input words into individual tokens and store it in an array | Using nltk.corpus.stopwords, remove the stopwords | Hint: See Module 1 - Assignment 4 &#39;Text Classification using Naive Bayes&#39; . Exercise 7: Use nltk package to find the most common words from the cleaned resume column [2 Marks] . Hint: . Use nltk.FreqDist | . stopword_list = nltk.corpus.stopwords.words(&#39;english&#39;) # YOUR CODE HERE to print the stop words in english language print(stopword_list) # set stopwords to english stop=set(stopwords.words(&#39;english&#39;)) # removing the stopwords def remove_stopwords(text, is_lower_case=False): # splitting strings into tokens (list of words) # YOUR CODE HERE to create &#39;tokens&#39; by applying word_tokenize() to &#39;text&#39; tokens = nltk.word_tokenize(text) tokens = [token.strip() for token in tokens] if is_lower_case: # filtering out the stop words filtered_tokens = [token for token in tokens if token not in stopword_list] else: filtered_tokens = [token for token in tokens if token.lower() not in stopword_list] filtered_text = &#39; &#39;.join(filtered_tokens) # YOUR CODE HERE to return &#39;filtered_text&#39; return filtered_text # apply function on review column resume_df[&#39;CleanedResume&#39;]=resume_df[&#39;CleanedResume&#39;].apply(remove_stopwords) . [&#39;i&#39;, &#39;me&#39;, &#39;my&#39;, &#39;myself&#39;, &#39;we&#39;, &#39;our&#39;, &#39;ours&#39;, &#39;ourselves&#39;, &#39;you&#39;, &#34;you&#39;re&#34;, &#34;you&#39;ve&#34;, &#34;you&#39;ll&#34;, &#34;you&#39;d&#34;, &#39;your&#39;, &#39;yours&#39;, &#39;yourself&#39;, &#39;yourselves&#39;, &#39;he&#39;, &#39;him&#39;, &#39;his&#39;, &#39;himself&#39;, &#39;she&#39;, &#34;she&#39;s&#34;, &#39;her&#39;, &#39;hers&#39;, &#39;herself&#39;, &#39;it&#39;, &#34;it&#39;s&#34;, &#39;its&#39;, &#39;itself&#39;, &#39;they&#39;, &#39;them&#39;, &#39;their&#39;, &#39;theirs&#39;, &#39;themselves&#39;, &#39;what&#39;, &#39;which&#39;, &#39;who&#39;, &#39;whom&#39;, &#39;this&#39;, &#39;that&#39;, &#34;that&#39;ll&#34;, &#39;these&#39;, &#39;those&#39;, &#39;am&#39;, &#39;is&#39;, &#39;are&#39;, &#39;was&#39;, &#39;were&#39;, &#39;be&#39;, &#39;been&#39;, &#39;being&#39;, &#39;have&#39;, &#39;has&#39;, &#39;had&#39;, &#39;having&#39;, &#39;do&#39;, &#39;does&#39;, &#39;did&#39;, &#39;doing&#39;, &#39;a&#39;, &#39;an&#39;, &#39;the&#39;, &#39;and&#39;, &#39;but&#39;, &#39;if&#39;, &#39;or&#39;, &#39;because&#39;, &#39;as&#39;, &#39;until&#39;, &#39;while&#39;, &#39;of&#39;, &#39;at&#39;, &#39;by&#39;, &#39;for&#39;, &#39;with&#39;, &#39;about&#39;, &#39;against&#39;, &#39;between&#39;, &#39;into&#39;, &#39;through&#39;, &#39;during&#39;, &#39;before&#39;, &#39;after&#39;, &#39;above&#39;, &#39;below&#39;, &#39;to&#39;, &#39;from&#39;, &#39;up&#39;, &#39;down&#39;, &#39;in&#39;, &#39;out&#39;, &#39;on&#39;, &#39;off&#39;, &#39;over&#39;, &#39;under&#39;, &#39;again&#39;, &#39;further&#39;, &#39;then&#39;, &#39;once&#39;, &#39;here&#39;, &#39;there&#39;, &#39;when&#39;, &#39;where&#39;, &#39;why&#39;, &#39;how&#39;, &#39;all&#39;, &#39;any&#39;, &#39;both&#39;, &#39;each&#39;, &#39;few&#39;, &#39;more&#39;, &#39;most&#39;, &#39;other&#39;, &#39;some&#39;, &#39;such&#39;, &#39;no&#39;, &#39;nor&#39;, &#39;not&#39;, &#39;only&#39;, &#39;own&#39;, &#39;same&#39;, &#39;so&#39;, &#39;than&#39;, &#39;too&#39;, &#39;very&#39;, &#39;s&#39;, &#39;t&#39;, &#39;can&#39;, &#39;will&#39;, &#39;just&#39;, &#39;don&#39;, &#34;don&#39;t&#34;, &#39;should&#39;, &#34;should&#39;ve&#34;, &#39;now&#39;, &#39;d&#39;, &#39;ll&#39;, &#39;m&#39;, &#39;o&#39;, &#39;re&#39;, &#39;ve&#39;, &#39;y&#39;, &#39;ain&#39;, &#39;aren&#39;, &#34;aren&#39;t&#34;, &#39;couldn&#39;, &#34;couldn&#39;t&#34;, &#39;didn&#39;, &#34;didn&#39;t&#34;, &#39;doesn&#39;, &#34;doesn&#39;t&#34;, &#39;hadn&#39;, &#34;hadn&#39;t&#34;, &#39;hasn&#39;, &#34;hasn&#39;t&#34;, &#39;haven&#39;, &#34;haven&#39;t&#34;, &#39;isn&#39;, &#34;isn&#39;t&#34;, &#39;ma&#39;, &#39;mightn&#39;, &#34;mightn&#39;t&#34;, &#39;mustn&#39;, &#34;mustn&#39;t&#34;, &#39;needn&#39;, &#34;needn&#39;t&#34;, &#39;shan&#39;, &#34;shan&#39;t&#34;, &#39;shouldn&#39;, &#34;shouldn&#39;t&#34;, &#39;wasn&#39;, &#34;wasn&#39;t&#34;, &#39;weren&#39;, &#34;weren&#39;t&#34;, &#39;won&#39;, &#34;won&#39;t&#34;, &#39;wouldn&#39;, &#34;wouldn&#39;t&#34;] . from nltk.probability import FreqDist from nltk.tokenize import word_tokenize # YOUR CODE HERE sent = resume_df.loc[0,&#39;CleanedResume&#39;] fdist = FreqDist(word.lower() for word in word_tokenize(sent)) . from nltk.probability import FreqDist from nltk.tokenize import word_tokenize # create a list of all words all_words = &#39; &#39;.join([word for word in resume_df[&#39;CleanedResume&#39;]]) # Tokenize all words tokenized_words = nltk.tokenize.word_tokenize(all_words) # create a frequency distribution which records the number of times each word has occured. fdist = FreqDist(tokenized_words) fdist_common = pd.DataFrame(fdist.most_common(50)) . #most_common_words = &#39; &#39;.join(review for review in fdist_common.loc[:,0]) plt.figure(figsize=(10,10)) WC = WordCloud(width=1000, height=500, max_words=500, min_font_size=5) most_common_words = &#39; &#39;.join(fdist_common.loc[:,0]) common_words = WC.generate(most_common_words) plt.imshow(common_words, interpolation=&#39;bilinear&#39;) plt.show . &lt;function matplotlib.pyplot.show&gt; . Exercise 8: Convert the categorical variable Category to a numerical feature and make a different column, which can be treated as the target variable [0.5 Mark] . Hint: Use sklearn.preprocessing.LabelEncoder() method . from sklearn.preprocessing import LabelEncoder # YOUR CODE HERE le = LabelEncoder() le.fit(resume_df[&#39;Category&#39;]) resume_df[&#39;labelledCategory&#39;] = le.transform(resume_df[&#39;Category&#39;]) . Feature Extraction . Exercise 9: Convert the text to feature vectors by applying tfidf vectorizer to the Label encoded category made above [2 Marks] . TF-IDFwill tokenize documents, learn the vocabulary, inverse document frequency weightings, and allow you to encode new documents . Hint: Use TfidfVectorizer(). . from sklearn.feature_extraction.text import TfidfVectorizer v = TfidfVectorizer() x = v.fit_transform(resume_df[&#39;CleanedResume&#39;]) . df_features = pd.DataFrame(x.toarray(), columns=v.get_feature_names()) . Naive Bayes Classifier . Exercise 10: Split the data into train and test sets. Apply Naive Bayes Classifier (MultinomialNB) and evaluate the model predictions [1 mark] . Hint: Use Vectorized features made above as X and Labelled category as y. . X_train, X_test, y_train, y_test = train_test_split(df_features, resume_df[&#39;labelledCategory&#39;], test_size = 0.2, random_state = 0) print(X_train.shape) print(X_test.shape) print(y_train.shape) print(y_test.shape) . (769, 6037) (193, 6037) (769,) (193,) . mnb=MultinomialNB() # fitting the NaiveBayes for count vectorizer mnb_resume = mnb.fit(X_train, y_train) print(&#39;MultinomialNB for Resume Train Data :&#39;,mnb_resume) # predicting the model mnb_resume_predict = mnb.predict(X_test) print(&#39;predictions for resume test data :&#39;, mnb_resume_predict[0:10]) # Accuracy of the model mnb_resume_score = accuracy_score(y_test, mnb_resume_predict) print(&quot;Accuracy mnb_cv_score :&quot;, mnb_resume_score) . MultinomialNB for Resume Train Data : MultinomialNB() predictions for resume test data : [20 14 15 17 15 14 10 14 15 23] Accuracy mnb_cv_score : 0.8860103626943006 . Optional: Create a Gradio based web interface to test and display the model predictions . !pip -qq install gradio . import gradio . def predict_category(text,mnb,v): # Cleaning the raw user input text = text.lower() text = data_cleaning(text) text = remove_stopwords(text) #transforming the data into model format text = [text] text = v.transform(input) label = mnb.predict(text) original_Category = le.inverse_transform(label) # Returning the original category return original_Category[0] . resume = gradio.inputs.Textbox(lines=10, placeholder=None, default=&quot;Data Science&quot;, label=&#39;Enter Resume &#39;) . out_label = gradio.outputs.Textbox(type=&quot;auto&quot;, label=&#39;Predicted Resume Category&#39;) . iface = gradio.Interface( fn = predict_category, inputs = resume, outputs = out_label) iface.launch(share=True) # Some error with the gradio, does&#39;t give me a prediction. Need to Debug . Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()` Running on public URL: https://58706.gradio.app This share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces) . (&lt;gradio.routes.App at 0x7f515cafe690&gt;, &#39;http://127.0.0.1:7868/&#39;, &#39;https://58706.gradio.app&#39;) . gr.Interface(fn=classify_image, inputs=sketchpad, outputs=label, interpretation=&quot;default&quot;, ).launch() . Report Analysis . Which method(s), other than TF-IDF could be used for text to vector conversion? | Discuss about the alpha, class_prior and fit_prior parameters in sklearn MultinomialNB | . Ans1 : Count Vectorizer can be used for text to vector conversion. Ans2 : alpha: Additive smoothing parameter (0 for no smoothing). In statistics, additive smoothing, also called Laplace smoothing, is a technique used to smooth categorical data. Given a set of observation counts $x = ⟨x1,x2,…,xd⟩$ from a d -dimensional multinomial distribution with N trials, a &quot;smoothed&quot; version of the counts gives the estimator: $θ^i=xi+αN+αd(i=1,…,d)$ , . where the smoothed count $x^i=Nθ^i$ and the &quot;pseudocount&quot; $α&gt;0$ is a smoothing parameter. $α=0$ corresponds to no smoothing. . fit_prior: Whether to learn class prior probabilities or not. If false, a uniform prior will be used. . class_prior: Prior probabilities of the classes. If specified the priors are not adjusted according to the data. . Dataset Source Reference: Resume dataset .",
            "url": "https://ai-girl.github.io/machine-learning/2022/06/05/M1_MP1_NB_Resume_Classification_Using_Naive_Bayes_Nasreen.html",
            "relUrl": "/2022/06/05/M1_MP1_NB_Resume_Classification_Using_Naive_Bayes_Nasreen.html",
            "date": " • Jun 5, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Linear Algebra - A review",
            "content": "$$ begin{aligned} y &amp;= x^2 &amp;= x cdot x end{aligned} $$ . Linear algebra is the study of vectors and certain rules to manipulate vectors. . In general, vectors are special objects that can be added together and multiplied by scalars to produce another object of the same kind. From an abstract mathematical viewpoint, any object that satisfies these two properties can be considered a vector. . Systems of Linear Equations . Matrices .",
            "url": "https://ai-girl.github.io/machine-learning/jupyter/2022/06/04/Linear-Algebra-Review.html",
            "relUrl": "/jupyter/2022/06/04/Linear-Algebra-Review.html",
            "date": " • Jun 4, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ai-girl.github.io/machine-learning/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ai-girl.github.io/machine-learning/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ai-girl.github.io/machine-learning/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ai-girl.github.io/machine-learning/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}